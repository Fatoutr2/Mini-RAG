# --- Interface utilisateur du Mini RAG avec Streamlit ---

# --- Importation des bibliotheques ---
import streamlit as st
from rag_core import load_documents, create_vector_store, retrieve
from sentence_transformers import SentenceTransformer
from openai import OpenAI
import os

# --- Client OpenRouter ---
client = OpenAI(
    api_key=os.getenv("OPENROUTER_API_KEY"),
    base_url="https://openrouter.ai/api/v1"
)

# --- Initialisation RAG ---
@st.cache_resource

# Cette fonction initialise tout le pipeline RAG.
# Elle est mise en cache pour √©viter de recalculer :
# - le chunking
# - les embeddings
# - l'index FAISS
# √† chaque interaction utilisateur.

def init_rag():
    # load_documents doit retourner chunks et metadata
    chunks, metadata = load_documents() # Chargement et d√©coupage des documents en chunks
    index = create_vector_store(chunks)  # Cr√©ation de l'index vectoriel FAISS √† partir des chunks
    return chunks, metadata, index # Retourne tous les √©l√©ments n√©cessaires au RAG

chunks, metadata, index = init_rag() # Appel de l'initialisation RAG

# --- Interface Utilisateur ---
st.set_page_config(page_title="Mini RAG", layout="wide")
st.title("ü§ñ Mini RAG")
st.markdown("Pose une question bas√©e sur les documents fournis.")

question = st.text_input("‚ùì Votre question")

# --- Traitement de la question posee par l'utilisateur ---
if st.button("üîç Rechercher") and question:

    # Embedding modele
    model = SentenceTransformer("all-MiniLM-L6-v2")
    
    # R√©cup√©ration des chunks pertinents
    results = retrieve(question, index, chunks, metadata, model, top_k=5)

    # Si aucun chunk pertinent n‚Äôest trouv√©
    if not results:
        st.error("‚ùå La r√©ponse n'existe pas dans les documents fournis.")
        st.stop()
    else:
        # Concat√©nation des chunks r√©cup√©r√©s
        context = "\n\n".join([r[0] for r in results])

        # Prompt pour le LLM
        prompt = f"""
Tu es un assistant STRICTEMENT limit√© au contexte ci-dessous.

R√àGLES ABSOLUES :
- Recherchez la reponse dans les documents forurnis.
- Si la r√©ponse n'est PAS dans le contexte, r√©pond exactement :
  "Je ne peux pas r√©pondre car l'information n'est pas pr√©sente dans les documents fournis."

Contexte:
{context}

Question: {question}
"""

        # Appel OpenRouter
        response = client.chat.completions.create(
            model="openai/gpt-4o-mini",
            messages=[{"role": "user", "content": prompt}]
        )

        # Affichage de la r√©ponse
        st.subheader("‚úÖ R√©ponse")
        st.write(response.choices[0].message.content)

        # Affichage des chunks utilis√©s
        st.subheader("üìö Chunks utilis√©s")
        for chunk, meta, _ in results:  # _ = distance
            with st.expander(f"{meta['source']} ‚Äì chunk {meta['chunk_id']}"):
                st.write(chunk)

